
---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "README-")
```

#corpuslingr: 

Some r functions for (1) quick web scraping and (2) corpus seach of complex grammatical constructions in context. 

The two sets of functions can be used in conjunction, or independently.  In theory, one could build a corpus of the days news (as a dataframe in text interchange format), annotate the corpus using `cleanNLP`, `spacyr`, or `udap`, and subsequently search the corpus for complex grammtical constructions utilizing search functionality akin to that made available in the [BYU suite of corpora]().

The package facilitates regex/CQL-based search across form, lemma, and detailed part-of-speech tags. Multi-term search is also supported.  Summary functions allow users to aggregate search results by text & token frequency, view search results in context (kwic), and create word embeddings/co-occurrence vectors for each search term. 

The collection of functions presented here is ideal for usage-based linguists and digital humanists interested in fine-grained search of moderately-sized (personal) corpora.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(cleanNLP)
library(corpuslingr) #devtools::install_github("jaytimm/corpuslingr")
```


##Web scraping functions

These functions .... There are other packages/means to scrape the web.  The two included here are designed for quick/easy search of headline news. And creation of tif corpus-object.  making subsequent annotation straightforward.  'Scrape news -> annotate -> search' in three or four steps.

Following grammatical constructions ~ day-to-day changes, eg. Search defaults to NULL, which smounts to national headlines.

###clr_web_gnews()
```{r}
dailyMeta <- corpuslingr::clr_web_gnews (search="New Mexico",n=30)

head(dailyMeta['titles'])
```


###clr_web_scrape()

This function ... takes the output of corp_web_gnews() (or any table with links to websites) ... and returns a 'tif as corpus df'
Text interchange formats.  Builds on top of boilerpipeR, XML, RCurl packages.

```{r}
nm_news <- dailyMeta %>% 
  corpuslingr::clr_web_scrape(link_var='links')
```


##Corpus preparation

###clr_prep_corpus
Also, PrepText(). Although, I think it may not work on TIF.
Hyphenated words and any excessive spacing in texts. Upstream solution.

```{r}
nm_news <- nm_news %>% mutate(text = corpuslingr::clr_prep_corpus (text, hyphenate = TRUE))
```


Using the ... `cleanNLP` package. 
```{r message=FALSE, warning=FALSE}
cleanNLP::cnlp_init_udpipe(model_name="english",feature_flag = FALSE, parser = "none") 
ann_corpus <- cleanNLP::cnlp_annotate(nm_news$text, as_strings = TRUE) 
```


###clr_set_corpus()

This function performs some cleaning ... It will ... any/all annotation types in theory.  Output, however, homogenizes column names to make things easier downstream. Naming conventions established in the `spacyr` package are adopted here.  The function performs two or three general tasks. Eliminates spaces.  Annotation form varies depending on the annotator, as different folks have different 

Adds tuples and their chraracter onsets/offsets. A fairly crude corpus querying language  

Lastly, the function splits corpus into a list of dataframes by doc_id.  This facilitates ... any easy solution to ...


```{r}
lingr_corpus <- ann_corpus$token %>%
  clr_set_corpus(doc_var='id', 
                  token_var='word', 
                  lemma_var='lemma', 
                  tag_var='pos', 
                  pos_var='upos',
                  sentence_var='sid',
                  NER_as_tag = FALSE)
```


###clr_desc_corpus() 

```{r}
corpuslingr::clr_desc_corpus(lingr_corpus)$corpus
```
```{r}
head(corpuslingr::clr_desc_corpus(lingr_corpus)$text)
```



##Search & aggregation functions

We also need to discuss special search terms, eg, `keyPhrase` and `nounPhrase`.  


###An in-house corpus querying language (CQL)

Should be 'copy and paste' at his point. See 'Corpus design' post. Tuples and complex corpus search.?

###clr_search_gramx()
```{r}
search1 <- "<_Vx> <up!>"

lingr_corpus %>%
  corpuslingr::clr_search_gramx(search=search1)%>%
  head ()
```

###clr_search_context()

This function allows ... output includes a list of data.frames.  `BOW` and `KWIC`

```{r}
search2 <- '<all!> <> <of!>'
found_egs <- corpuslingr::clr_search_context(search=search2,corp=lingr_corpus,LW=5, RW = 5)
```



```{r}
clr_nounphrase
```


###clr_get_freqs()
```{r}
lingr_corpus %>%
  corpuslingr::clr_search_gramx(search=search1)%>%
  corpuslingr::clr_get_freq(agg_var = 'lemma')%>%
  head()
```


###clr_context_kwic()
```{r}
search4 <- "<_Jx> <and!> <_Jx>"

corpuslingr::clr_search_context(search=search4,corp=lingr_corpus,LW=5, RW = 5)%>%
  corpuslingr::clr_context_kwic()%>%
  head()
```


###clr_context_bow()
Vector space model, or word embedding
```{r}
corpuslingr::clr_search_context(search=search4,corp=lingr_corpus,LW=5, RW = 5)%>%
  corpuslingr::clr_context_bow()%>%
  head()
```


###clr_search_keyphrases()

most of this is described more thoroughly in this [post](https://www.jtimm.net/blog/keyphrase-extraction-from-a-corpus-of-texts/).

The function leverages `clr_search_gramx()` .... uses tf-idf weights to extract keyphrases from each text comprising corpus.  The user can specify ... 

```{r}
clr_keyphrase
```



```{r}
lingr_corpus %>%
  corpuslingr::clr_search_keyphrases(n=5, key_var ='lemma', flatten=TRUE,jitter=TRUE)%>%
  head()
```

?Reference corpus.  
~Perhaps using SOTU.

##Multi-term search

```{r}
#multi-search <- c("")
search6 <- "<_xNP> (<wish&> |<hope&> |<believe&> )"
```


##Corpus workflow with corpuslingr, cleanNLP, & tidy

```{r eval=FALSE, message=FALSE, warning=FALSE}
corpuslingr::clr_web_gnews(search="New Mexico",n=30) %>%
  corpuslingr::clr_web_scrape(link_var='links') %>%
  cleanNLP::cnlp_annotate(as_strings = TRUE) %>%
  corpuslingr::clr_set_corpus(doc_var='id', 
                  token_var='word', 
                  lemma_var='lemma', 
                  tag_var='pos', 
                  pos_var='upos',
                  sentence_var='sid',
                  NER_as_tag = FALSE) %>%
  corpuslingr::clr_search_context(search=search2,LW=5, RW = 5)%>%
  corpuslingr::clr_context_kwic()
```



