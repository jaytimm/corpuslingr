
---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "README-")
```

##corpuslingr: 

Some r functions for (1) quick web scraping and (2) corpus seach of complex grammatical constructions. 

High Level utility. Hypothetical workflows. Independently or in conjunction.  Academic linguists and digital humanists.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(cleanNLP)
library(stringi)
library(corpuslingr) #devtools::install_github("jaytimm/corpuslingr")
```


##Web scraping functions

These functions .... There are other packages/means to scrape the web.  The two included here are designed for quick/easy search of headline news. And creation of tif corpus-object.  making subsequent annotation straightforward.  'Scrape news -> annotate -> search' in three or four steps.

Following grammatical constructions ~ day-to-day changes, eg. Search defaults to NULL, which smounts to national headlines.

###corp_web_gnews()
```{r}
dailyMeta <- corpuslingr::corp_web_gnews (search="New Mexico",n=30)

head(dailyMeta['titles'])
```


###corp_web_scrape()

This function ... takes the output of corp_web_gnews() (or any table with links to websites) ... and returns a 'tif as corpus df'
Text interchange formats.  Builds on top of boilerpipeR, XML, RCurl packages.

```{r}
nm_news <- dailyMeta %>% 
  corpuslingr::corp_web_scrape(link_var='links')
```


##Corpus preparation

###corp_prep_corpus
Also, PrepText(). Although, I think it may not work on TIF.
Hyphenated words and any excessive spacing in texts. Upstream solution.

```{r}
nm_news <- nm_news %>% mutate(text = corpuslingr::corp_prep_corpus (text, hyphenate = TRUE))
```


Using the ... `cleanNLP` package. 
```{r message=FALSE, warning=FALSE}
cleanNLP::cnlp_init_udpipe(model_name="english",feature_flag = FALSE, parser = "none") 
ann_corpus <- cleanNLP::cnlp_annotate(nm_news$text, as_strings = TRUE) 
```


###corp_set_corpus()

This function performs some cleaning ... It will ... any/all annotation types in theory.  Output, however, homogenizes column names to make things easier downstream. Naming conventions established in the `spacyr` package are adopted here.  The function performs two or three general tasks. Eliminates spaces.  Annotation form varies depending on the annotator, as different folks have different 

Adds tuples and their chraracter onsets/offsets. A fairly crude corpus querying language  

Lastly, the function splits corpus into a list of dataframes by doc_id.  This facilitates ... any easy solution to ...


```{r}
lingr_corpus <- ann_corpus$token %>%
  corp_set_corpus(doc_var='id', 
                  token_var='word', 
                  lemma_var='lemma', 
                  tag_var='pos', 
                  pos_var='upos',
                  sentence_var='sid',
                  NER_as_tag = FALSE)
```


###GetDocDesc() 


```{r}
corpuslingr::GetDocDesc(lingr_corpus)$corpus
```
```{r}
head(corpuslingr::GetDocDesc(lingr_corpus)$text)
```



##Search & aggregation functions

We also need to discuss special search terms, eg, `keyPhrase` and `nounPhrase`.  


###An in-house corpus querying language (CQL)

Should be 'copy and paste' at his point. See 'Corpus design' post. Tuples and complex corpus search.?

###SimpleSearch()
```{r}
search1 <- "<_Vx> <up!>"

lingr_corpus %>%
  corpuslingr::SimpleSearch(search=search1)%>%
  head ()
```

###GetContexts()

This function allows ... output includes a list of data.frames.  `BOW` and `KWIC`

```{r}
search2 <- '<all!> <> <of!>'
corpuslingr::GetContexts(search=search2,corp=lingr_corpus,LW=5, RW = 5)%>%
  corpuslingr::GetKWIC()
```



```{r}
nounPhrase
```


###GetSearchFreqs()
```{r}
lingr_corpus %>%
  corpuslingr::SimpleSearch(search=search1)%>%
  corpuslingr::GetSearchFreqs(aggBy = 'lemma')%>%
  head()
```


###GetKWIC()
```{r}
search4 <- "<_Jx> <and!> <_Jx>"

corpuslingr::GetContexts(search=search4,corp=lingr_corpus,LW=5, RW = 5)%>%
  corpuslingr::GetKWIC()%>%
  head()
```

###GetBOW()
Vector space model, or word embedding


###GetKeyphrases()

most of this is described more thoroughly in this [post](https://www.jtimm.net/blog/keyphrase-extraction-from-a-corpus-of-texts/).

The function leverages `SimpleSearch()` .... uses tf-idf weights to extract keyphrases from each text comprising corpus.  The user can specify ... 

```{r}
keyPhrase
```
```{r}
lingr_corpus %>%
  GetKeyPhrases(n=5, key_var ='lemma', flatten=TRUE,jitter=TRUE)%>%
  head()
```

?Reference corpus.  
~Perhaps using SOTU.

##Multi-term search

```{r}
#multi-search <- c("")
search6 <- "<_xNP> (<wish&> |<hope&> |<believe&> )"
```


##Corpus workflow with corpuslingr, cleanNLP, & tidy

```{r eval=FALSE, message=FALSE, warning=FALSE}
corpuslingr::GetGoogleNewsMeta (search="New Mexico",n=30) %>%
  corpuslingr::GetWebTexts(link_var='links') %>%
  cleanNLP::cnlp_annotate(as_strings = TRUE) %>%
  corpuslingr::SetSearchCorpus(doc_var='id', 
                  token_var='word', 
                  lemma_var='lemma', 
                  tag_var='pos', 
                  pos_var='upos',
                  sentence_var='sid',
                  NER_as_tag = FALSE) %>%
  corpuslingr::GetContexts(search=search2,LW=5, RW = 5)%>%
  corpuslingr::GetKWIC()
```



