
---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "README-")
```

##corpuslingr: 

Some r functions for quick web scraping and corpus seach of complex grammtical constructions. Works in conjunction with `spacyr` package. 
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
devtools::install_github("jaytimm/corpuslingr")
#devtools::install_github("jaytimm/corpusdatr")
library(corpuslingr)
library(corpusdatr)
library(knitr)
```

##Web scraping functions
###`GetGoogleNewsMeta()`
```{r}
#dailyMeta <- corpuslingr::GetGoogleNewsMeta (search="New Mexico", n=5)
dailyMeta <- corpuslingr::GetGoogleNewsMeta (n=15)

head(dailyMeta['titles'])
```

We need to sort out meta with sites that are actually scraped.
Also, re-try "article" verion of boilerpipeR.

###`GetWebTexts()`
```{r}
txts <- dailyMeta$links  %>% 
  GetWebTexts()

substr(txts[1:5],1, 50)
```


##Corpus preparation
###`PrepAnnotation()`
```{r eval=FALSE}
annotations <- txts  %>%
  lapply(spacyr::spacy_parse,tag=TRUE)%>%
  corpuslingr::PrepAnnotation()
```

Output consists of a list of dataframes.  Distinct from `spacyr` output.
```{r}
gnews <- corpusdatr::gnews11_20_17
```

###`GetDocDesc()` 
```{r}
head(GetDocDesc(gnews))
```


##Search function and aggregate functions.
###`GetContexts()`
```{r}
search1 <- "<_Vx> <_IN>"

found <- corpuslingr::GetContexts(search=search1,corp=gnews,LW=5, RW = 5)
```


###`GetSearchFreqs()`
```{r}
corpuslingr::GetSearchFreqs(found)[[1]]
```

###`GetKWIC()`
```{r}
search2 <- "<_Jx> <and!> <_Jx>"

corpuslingr::GetContexts(search=search2,corp=gnews,LW=5, RW = 5)%>%
  corpuslingr::GetKWIC()%>%
  data.frame()%>%
  select(doc_id,cont)%>%
  mutate(cont=gsub("<mark>|</mark>","||",cont))%>%
  knitr::kable("markdown")
```

###`GetBOW()`
Vector space model, or word embedding
```{r}
search3 <- "<Trump!>"

corpuslingr::GetContexts(search=search3,corp=gnews,LW=15, RW = 15)%>%
  corpuslingr::GetBOW(contentOnly=TRUE)%>%
  data.frame()%>%
  slice(1:10)%>%
  ggplot(aes(x=reorder(lemma,n), y=n)) + 
    geom_bar(stat="identity", width=.5, fill="tomato3") +  
    coord_flip()+
    theme_bw()

##How would we get Noun Phrases from a BOW?
```



##Multi-term search

```{r}
#multi-search <- c("")
```


##Corpus workflow
```{r eval=FALSE, message=FALSE, warning=FALSE}
search4 <- "<_xNP> (<wish&> |<hope&> |<believe&> )"

dailyMeta$links  %>% 
  corpuslingr::GetWebTexts()%>%
  lapply(spacyr::spacy_parse,tag=TRUE)%>%
  corpuslingr::PrepAnnotation()%>%
  corpuslingr::GetContexts(search=search4,corp=.,LW=10, RW = 10)%>%
  corpuslingr::GetSearchFreqs(found)[[1]]
```



