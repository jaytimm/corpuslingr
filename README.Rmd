
---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "README-")
```

##corpuslingr: 

Corpus work flow.  
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
#devtools::install_github("jaytimm/corpuslingr")
library(corpuslingr)
```

```{r message=FALSE, warning=FALSE}
library(spacyr)
spacy_initialize()
#spacy_initialize(python_executable = "C:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python36\\python.exe")
```

##Web-based functions --  super simple
```{r}
#dailyMeta <- corpuslingr::GetGoogleNewsMeta (search="New Mexico", n=5)
dailyMeta <- corpuslingr::GetGoogleNewsMeta (n=15)

head(dailyMeta[1:2])
```

We need to sort out meta with sites that are actually scraped.
Also, re-try "artcle" verion of boilerpipeR.

```{r}
txts <- dailyMeta$links  %>% 
  GetWebTexts()

substr(txts[1:5],1, 100)
```


Can be used to in a pipe laong with a corpus annotator, in this case `spacyr`...`GetWebTexts` a generic webscraping function





```{r}
annotations <- txts  %>%
  lapply(spacyr::spacy_parse,tag=TRUE)%>%
  corpuslingr::PrepAnnotation()
```

Output consists of a list of dataframes.  Distinct from `spacyr` output.

```{r}
head(annotations[[1]])
```


```{r}
head(GetDocDesc(annotations))
```


##Search function and aggregate functions.




GetSearchFreqs()
GetKWIC()
GetBOW()

Allows for multiple search terms...


Should add date. Demonstrate currency.
```{r message=FALSE, warning=FALSE}
library(knitr)

term <- "<_Jx> <and!> <_Jx>"

annotations%>%
  corpuslingr::GetContexts(search=term,corp=., LW=5, RW = 5)%>%
  corpuslingr::GetKWIC()%>%
  data.frame()%>%
  select(doc_id,cont)%>%
  mutate(cont=gsub("<mark>|</mark>","||",cont))%>%
  kable("markdown")
```

render("C:\\Users\\jason\\Google Drive\\GitHub\\packages\\corpuslingr\\README.rmd")


