
---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "README-")
```

#"Automates many of the tasks associated with quantitative discourse analysis of
transcripts containing discourse including frequency counts of sentence types,
words, sentences, turns of talk, syllables and other assorted analysis tasks. The
package provides parsing tools for preparing transcript data. Many functions enable
the user to aggregate data by any number of grouping variables, providing analysis
and seamless integration with other R packages that undertake higher level analysis
and visualization of text. This affords the user a more efficient and targeted
analysis. 'qdap' is designed for transcript analysis, however, many functions are
applicable to other areas of Text Mining/ Natural Language Processing."


Imports chron, dplyr (>= 0.3), gdata, gender (>= 0.5.1), ggplot2 (>=
2.1.0), grid, gridExtra, igraph, methods, NLP, openNLP (>=
0.2-1), parallel, plotrix, RCurl, reports, reshape2, scales,
stringdist, tidyr, tm (>= 0.7.2), tools, venneuler, wordcloud,
xlsx, XML

From old-'Annotated':
Here we add a few features to these standard annotations, and introduce a basic corpus querying language, to enable fine-grained corpus search akin to the functionality of search made available in the BYU [suite of corpora](https://corpus.byu.edu/).  Via ... string-based search akin to the type of search made avaialble in Python's Natural Language Toolkit (NLTK), for example. -- 'Complex search' as a potentially non-contiguous pattern occurring across multiple features of annotation. -- Which aligns with def of gramx


#corpuslingr: an R package for complex corpus search & web-based corpus creation

Define complex corpus search.  Patterns comprised of ... one or more than one search term. comprised of contiguous and/or non-contiguous elements.  comprised of fixed and/or non-fixed.  comprised of some combination of lemma/form/part-of-speech.

This package:

* facilitates regex/CQL-based search across form, lemma, and detailed part-of-speech tags. Multi-term search is also supported.  Summary functions allow users to aggregate search results by text & token frequency, view search results in context (kwic), and create word embeddings/co-occurrence vectors for each search term.

* facilitates quick/easy web scraping of news sources, as a dataframe in text interchange format...

subsequently search the corpus for complex grammatical constructions utilizing search functionality akin to that made available in the [BYU suite of corpora](https://corpus.byu.edu/).

The collection of functions presented here is ideal for usage-based linguists and digital humanists interested in fine-grained search of moderately-sized corpora.

Here, we walk through a simple workflow from corpus creation using `corpuslingr`, corpus annotation using the `cleanNLP` package, and annotated corpus search using `corpuslingr`.  Bookends.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(cleanNLP)
library(corpuslingr) #devtools::install_github("jaytimm/corpuslingr")
```


##Web scraping functions

The two web-based functions presented here more or less work in tandem.  

###clr_web_gnews()

The first, `clr_web_gnews`, simply pulls metadata based on user-specified search paramaters from the GoogleNews RSS feed.  

```{r}
dailyMeta <- corpuslingr::clr_web_gnews (search="New Mexico",n=30)
```

Metadata include:
```{r echo=FALSE}
colnames(dailyMeta)
```

First six article titles:
```{r}
head(dailyMeta['titles'])
```


###clr_web_scrape()

The second web-based function, `clr_web_scrape`, scrapes text from the web addresses included in the output from `clr_web_gnews` (or any vector that contains web addresses).  The function returns a [TIF](https://github.com/ropensci/tif#text-interchange-formats)-compliant dataframe, with each scraped text represented as a single row.  Metadata from output of `clr_web_gnews` are also included.

Both functions depend on functionality made available in the `boilerpipeR`, `XML`, and `RCurl` packages.

```{r}
nm_news <- dailyMeta %>% 
  corpuslingr::clr_web_scrape(link_var='links')
```


##Corpus preparation & annotation

###clr_prep_corpus

This function performs two tasks.  It elminates unnecessary whitespace from the text column of a corpus dataframe object.  Additionally, it attempts to trick annotators into treating hyphenated words as a single token.  With the exception of Stanford's CoreNLP (via `cleanNLP`), annotators tend to treat hyphenated words as multiple word tokens.  For linguists interested in word formation processes, eg, this is disappointing. There is likley a less hacky way to do this.

```{r}
nm_news <- nm_news %>% mutate(text = corpuslingr::clr_prep_corpus (text, hyphenate = TRUE))
```


###Annotate via cleanNLP and udpipe
For demo purposes, we use `udpipe` (via `cleanNLP`) to annotate the corpus dataframe object. The `cleanNLP` package is fantastic -- the author has aggregated three different annotators (spacy, CoreNLP, and `udpipe`) into one convenient pacakge.  There are pros/cons with each annotator; we won't get into these here.     

A benefit of `udpipe` is that it is dependency-free, making it super useful for classroom and demo purposes.
 
```{r message=FALSE, warning=FALSE}
cleanNLP::cnlp_init_udpipe(model_name="english",feature_flag = FALSE, parser = "none") 
ann_corpus <- cleanNLP::cnlp_annotate(nm_news$text, as_strings = TRUE) 
```


###clr_set_corpus()

This function prepares the annotated corpus for complex, tuple-based search.  Tuples are created, taking the form `<token~lemma~pos>`; tuple onsets/offsets are also set. Annotation output is homogenized, including column names, making text processing easier 'downstream.' Naming conventions established in the `spacyr` package are adopted here.    

Lastly, the function splits the corpus into a list of dataframes by document.  This is ultimately a search convenience.

```{r}
lingr_corpus <- ann_corpus$token %>%
  clr_set_corpus(doc_var='id', 
                  token_var='word', 
                  lemma_var='lemma', 
                  tag_var='pos', 
                  pos_var='upos',
                  sentence_var='sid',
                  NER_as_tag = FALSE)
```


###clr_desc_corpus() 

A simple function for describing the corpus. As can be noted, not all of the user-specified (n=30) links were successfully scraped.  Not all websites care to be scraped.
```{r}
corpuslingr::clr_desc_corpus(lingr_corpus)$corpus
```

Text-based descritpives:
```{r}
head(corpuslingr::clr_desc_corpus(lingr_corpus)$text)
```



##Search & aggregation functions


###A corpus querying language (CQL)

A fairly crude corpus querying language is utilized/included in the package.

The CQL presented here consists of four basic elements.  Individual search components are enclosed with `<>`, lemma search is specified using `&`, token search is specified using `!`, and part-of-speech search is specified using `_`.  Additionally, parts-of-speech can be made generic/universal with the suffix `x`. So, a search for all nouns forms (NN, NNS, NNP, NNPS) would be specified by `_Nx`. All other regex rules apply.

###clr_search_gramx()

Search for all instantiaions of a particular lexical pattern/grammatical construction devoid of context.  This function enables fairly quick search.
```{r}
search1 <- "<_Vx> <up!>"

lingr_corpus %>%
  corpuslingr::clr_search_gramx(search=search1)%>%
  head ()
```


###clr_get_freqs()

A simple function for calculating text and token frequencies of search term(s).  The `agg_var` parameter allows the user to specify how frequency counts are aggregated.

Note: Generic nounphrases can be include as a search term.  The regex for a generic nounphrase is below, and can be specified in the CQL query using `_NXP`.
```{r}
clr_nounphrase
```


```{r}
search2 <- "<_NXP> <_Vx> <to!> <_Vx>"

lingr_corpus %>%
  corpuslingr::clr_search_gramx(search=search2)%>%
  corpuslingr::clr_get_freq(agg_var = 'token')%>%
  head()
```

###clr_search_context()

A function that returns search terms with user-specified left and right contexts (`LW` and `RW`).  Output includes a list of two dataframes: a `BOW` (bag-of-words) dataframe object and a `KWIC` (keyword in context) dataframe object.  

```{r}
search3 <- '<_Jx> <and!> <_Jx>'

found_egs <- corpuslingr::clr_search_context(search=search3,corp=lingr_corpus,LW=5, RW = 5)
```


###clr_context_kwic()

Access `KWIC` object:
```{r}
found_egs %>%
  corpuslingr::clr_context_kwic()%>%
  head()
```


###clr_context_bow()

Access `BOW` object:

```{r}
search3 <- c('<Santa&> <Fe&>','<Albuquerque&>')

corpuslingr::clr_search_context(search=search3,corp=lingr_corpus,LW=10, RW = 10)%>%
  corpuslingr::clr_context_bow(content_only=TRUE,agg_var=c('searchLemma','lemma'))%>%
  head()
```


###clr_search_keyphrases()

Function for extracting keyphrases from each text comprising a corpus based on tf-idf weights.  The methods and logic underlying this function are described in more detail [here](https://www.jtimm.net/blog/keyphrase-extraction-from-a-corpus-of-texts/).

The regex for keyphrase search:
```{r}
clr_keyphrase
```


The use can specify the number of keyphrases to extract, 

For super small corpora (as our demo corpus is), results will likely be less favorable.
```{r}
lingr_corpus %>%
  corpuslingr::clr_search_keyphrases(n=5, key_var ='lemma', flatten=TRUE,jitter=TRUE)%>%
  head()
```


##Multi-term search

```{r}
#multi-search <- c("")
search6 <- "<_xNP> (<wish&> |<hope&> |<believe&> )"
```


##Corpus workflow with corpuslingr, cleanNLP, & magrittr

```{r eval=FALSE, message=FALSE, warning=FALSE}
corpuslingr::clr_web_gnews(search="New Mexico",n=30) %>%
  corpuslingr::clr_web_scrape(link_var='links') %>%
  cleanNLP::cnlp_annotate(as_strings = TRUE) %>%
  corpuslingr::clr_set_corpus(doc_var='id', 
                  token_var='word', 
                  lemma_var='lemma', 
                  tag_var='pos', 
                  pos_var='upos',
                  sentence_var='sid',
                  NER_as_tag = FALSE) %>%
  corpuslingr::clr_search_context(search=search2,LW=5, RW = 5)%>%
  corpuslingr::clr_context_kwic()
```



